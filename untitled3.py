# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15mHmpKcKzPc7sH3JBuxt0SS3y3j-Vr7u
"""

# 安装依赖（仅首次运行需要）
!pip install rasterio matplotlib tqdm

# 挂载 Google Drive（如果在 Colab 上运行）
from google.colab import drive
drive.mount('/content/drive')

# ========= IMPORTS =========
import os
import numpy as np
import rasterio
import matplotlib.pyplot as plt
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import gc
from datetime import datetime

# ========= 设置参数 =========
# 根目录（包含7500个子文件夹）
root_folder = "/content/drive/My Drive/lake100_orignal"
# 日志文件路径
log_file_path = os.path.join(root_folder, "processing_log.txt")
# 每批处理多少个文件夹（建议 500–1000）
batch_size = 1000
# 同时处理几个（线程数，建议 ≤4）
max_threads = 3

# ========= 初始化日志 =========
with open(log_file_path, "w") as log_file:
    log_file.write(f"📘 中值合成处理日志 - {datetime.now()}\n\n")

# ========= 查找所有含 .tif 文件的子文件夹 =========
all_folders = []
for root, dirs, files in os.walk(root_folder):
    if any(f.endswith(".tif") for f in files):
        all_folders.append(root)

print(f"📂 共发现 {len(all_folders)} 个可处理的子文件夹")

# ========= 单个文件夹处理逻辑 =========
def process_folder(tif_folder):
    try:
        # 跳过已处理的
        output_path = os.path.join(tif_folder, "median_composite.png")
        if os.path.exists(output_path):
            return f"✅ 已存在跳过: {tif_folder}"

        tif_files = [os.path.join(tif_folder, f) for f in os.listdir(tif_folder) if f.endswith(".tif")]
        if not tif_files:
            return f"⚠️ 无 .tif 文件跳过: {tif_folder}"

        with rasterio.open(tif_files[0]) as src:
            band_count = src.count

        stack = []
        for tif in tif_files:
            with rasterio.open(tif) as src:
                img = src.read().astype(np.float32)
                stack.append(img)

        stack = np.stack(stack, axis=0)
        median_composite = np.median(stack, axis=0)

        rgb = median_composite[:3]
        rgb = rgb / np.max(rgb)
        rgb = np.clip(rgb, 0, 1)
        rgb_img = np.transpose(rgb, (1, 2, 0))

        plt.imsave(output_path, rgb_img)

        # 内存清理
        del stack, median_composite, rgb, rgb_img
        gc.collect()
        plt.close("all")

        return f"✅ 成功处理: {tif_folder}"

    except Exception as e:
        return f"❌ 处理失败: {tif_folder}\n错误: {str(e)}"

# ========= 分批并行处理 =========
def process_in_batches(folders, batch_size=1000, max_threads=3):
    total = len(folders)
    for start in range(0, total, batch_size):
        batch = folders[start:start + batch_size]
        print(f"\n🚀 正在处理第 {start//batch_size + 1} 批（共 {len(batch)} 个文件夹）")

        with ThreadPoolExecutor(max_workers=max_threads) as executor, open(log_file_path, "a") as log_file:
            futures = {executor.submit(process_folder, folder): folder for folder in batch}
            for future in tqdm(as_completed(futures), total=len(futures), desc="处理进度"):
                result = future.result()
                print(result)
                log_file.write(result + "\n")

# ========= 启动处理 =========
process_in_batches(all_folders, batch_size=batch_size, max_threads=max_threads)
print(f"\n🎉 所有批次处理完毕，日志保存于：{log_file_path}")

# ========= IMPORTS =========
import os
import numpy as np
import rasterio
import matplotlib.pyplot as plt
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import gc
from datetime import datetime

# ========= 设置参数 =========
# 只处理这个文件夹里的子文件夹
target_folder = "/content/drive/My Drive/lake100_orignal/no_png"
# 日志文件路径
log_file_path = os.path.join(target_folder, "processing_log.txt")
# 每批处理多少个文件夹（建议 500–1000）
batch_size = 1000
# 同时处理几个（线程数，建议 ≤4）
max_threads = 3

# ========= 初始化日志 =========
with open(log_file_path, "w") as log_file:
    log_file.write(f"📘 中值合成处理日志 - {datetime.now()}\n\n")

# ========= 获取子文件夹中含 .tif 文件的文件夹 =========
all_folders = []
for sub in os.listdir(target_folder):
    sub_path = os.path.join(target_folder, sub)
    if os.path.isdir(sub_path):
        tif_files = [f for f in os.listdir(sub_path) if f.endswith(".tif")]
        if tif_files:
            all_folders.append(sub_path)

print(f"📂 在 'no_png' 中共发现 {len(all_folders)} 个含 .tif 的子文件夹")

# ========= 单个文件夹处理逻辑 =========
def process_folder(tif_folder):
    try:
        output_path = os.path.join(tif_folder, "median_composite.png")
        if os.path.exists(output_path):
            return f"✅ 已存在跳过: {tif_folder}"

        tif_files = [os.path.join(tif_folder, f) for f in os.listdir(tif_folder) if f.endswith(".tif")]
        if not tif_files:
            return f"⚠️ 无 .tif 文件跳过: {tif_folder}"

        with rasterio.open(tif_files[0]) as src:
            band_count = src.count

        stack = []
        for tif in tif_files:
            with rasterio.open(tif) as src:
                img = src.read().astype(np.float32)
                stack.append(img)

        stack = np.stack(stack, axis=0)
        median_composite = np.median(stack, axis=0)

        rgb = median_composite[:3]
        rgb = rgb / np.max(rgb)
        rgb = np.clip(rgb, 0, 1)
        rgb_img = np.transpose(rgb, (1, 2, 0))

        plt.imsave(output_path, rgb_img)

        # 内存清理
        del stack, median_composite, rgb, rgb_img
        gc.collect()
        plt.close("all")

        return f"✅ 成功处理: {tif_folder}"

    except Exception as e:
        return f"❌ 处理失败: {tif_folder}\n错误: {str(e)}"

# ========= 分批并行处理 =========
def process_in_batches(folders, batch_size=1000, max_threads=3):
    total = len(folders)
    for start in range(0, total, batch_size):
        batch = folders[start:start + batch_size]
        print(f"\n🚀 正在处理第 {start//batch_size + 1} 批（共 {len(batch)} 个文件夹）")

        with ThreadPoolExecutor(max_workers=max_threads) as executor, open(log_file_path, "a") as log_file:
            futures = {executor.submit(process_folder, folder): folder for folder in batch}
            for future in tqdm(as_completed(futures), total=len(futures), desc="处理进度"):
                result = future.result()
                print(result)
                log_file.write(result + "\n")

# ========= 启动处理 =========
process_in_batches(all_folders, batch_size=batch_size, max_threads=max_threads)
print(f"\n🎉 所有批次处理完毕，日志保存于：{log_file_path}")

# ========= IMPORTS =========
import os
import numpy as np
import rasterio
from imageio import imwrite
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import gc
from datetime import datetime
import psutil

# ========= 设置参数 =========
# 目标目录
target_folder = "/content/drive/My Drive/lake100_orignal"
# 日志路径
log_file_path = os.path.join(target_folder, "processing_log.txt")
# 每批处理多少个文件夹（建议 ≤10）
batch_size = 5
# 同时处理几个（线程数，建议 ≤2）
max_threads = 1

# ========= 初始化日志 =========
with open(log_file_path, "w") as log_file:
    log_file.write(f"📘 中值合成处理日志 - {datetime.now()}\n\n")

# ========= 获取子文件夹中含 .tif 文件的文件夹 =========
all_folders = []
for sub in os.listdir(target_folder):
    sub_path = os.path.join(target_folder, sub)
    if os.path.isdir(sub_path):
        tif_files = [f for f in os.listdir(sub_path) if f.endswith(".tif")]
        if tif_files:
            all_folders.append(sub_path)

print(f"📂 在 'no_png' 中共发现 {len(all_folders)} 个含 .tif 的子文件夹")

# ========= 单个文件夹处理逻辑 =========
def process_folder(tif_folder):
    try:
        output_path = os.path.join(tif_folder, "median_composite.png")
        if os.path.exists(output_path):
            return f"✅ 已存在跳过: {tif_folder}"

        tif_files = [os.path.join(tif_folder, f) for f in os.listdir(tif_folder) if f.endswith(".tif")]
        if not tif_files:
            return f"⚠️ 无 .tif 文件跳过: {tif_folder}"

        # 打印内存状态
        mem = psutil.virtual_memory()
        print(f"🧠 当前可用内存: {mem.available / 1024**3:.2f} GB - {tif_folder}")

        sample_shape = None
        images = []

        for tif in tif_files:
            with rasterio.open(tif) as src:
                img = src.read().astype(np.float32)
                if sample_shape is None:
                    sample_shape = img.shape
                if img.shape != sample_shape:
                    continue
                images.append(img)

        if len(images) == 0:
            return f"⚠️ 所有图像尺寸不一致跳过: {tif_folder}"

        stack = np.stack(images, axis=0)
        del images
        gc.collect()

        median_composite = np.median(stack, axis=0)
        del stack
        gc.collect()

        rgb = median_composite[:3]
        rgb = rgb / np.max(rgb)
        rgb = np.clip(rgb, 0, 1)
        rgb_img = np.transpose(rgb, (1, 2, 0))
        imwrite(output_path, (rgb_img * 255).astype(np.uint8))

        del median_composite, rgb, rgb_img
        gc.collect()

        return f"✅ 成功处理: {tif_folder}"

    except Exception as e:
        gc.collect()
        return f"❌ 处理失败: {tif_folder}\n错误: {str(e)}"

# ========= 分批并行处理 =========
def process_in_batches(folders, batch_size=5, max_threads=1):
    total = len(folders)
    for start in range(0, total, batch_size):
        batch = folders[start:start + batch_size]
        print(f"\n🚀 正在处理第 {start//batch_size + 1} 批（共 {len(batch)} 个文件夹）")

        with ThreadPoolExecutor(max_workers=max_threads) as executor, open(log_file_path, "a") as log_file:
            futures = {executor.submit(process_folder, folder): folder for folder in batch}
            for future in tqdm(as_completed(futures), total=len(futures), desc="处理进度"):
                result = future.result()
                print(result)
                log_file.write(result + "\n")

# ========= 启动处理 =========
process_in_batches(all_folders, batch_size=batch_size, max_threads=max_threads)
print(f"\n🎉 所有批次处理完毕，日志保存于：{log_file_path}")